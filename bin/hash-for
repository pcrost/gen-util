#!/usr/bin/python
#
# A preprocessor that expands textual for loops. e.g:
#
# #for A(0, 1, 2)
# some code A
# #endfor
#
# expands to:
#
# some code 0
# some code 1
# some code 2
#
# Allows both nesting and all-on-one-line. E.g:
#
# #for A(0, 1) #for B(A, B) ex A B #endfor #endfor
#
# ex 0 0 ex 0 B ex 1 1 ex 1 B
#
# Prefixing of suffixing the substitutable with _ will cause
# automatic token pasting (which can be naughtily used to autogenerate
# macros themselves). Tokens paste in the direction of the _. You can
# paste both ways at once, E.g:
#
# #for A(X, Y) #define FOO_ _A BAR_ _A_ _BAZ
# #endfor
#
# #define FOO_X BAR_X_BAZ
# #define FOO_Y BAR_Y_BAZ
#
# If there are two consecutive substitutables then if either one (or both)
# specifies a paste to the other, then the tokens will be pasted.
#
# Minimal effort gone into syntax checking

import re
import sys

state = "off"

obufs = [[]]
finds = []
replaces = []

line_nr = 0

def parse_error(cond, expected, before):
    if (cond):
        print "line " + str(line_nr) + ": expected " + expected + " before " + before
        exit(1)

def is_keyword(token):
    return token == "#for" or token == "#endfor" or token == "(" or token == ")"

def char_is_white(c):
    return c == " " or c == "\t" or c == "\n" or c == "\r" or c == "\0"

def string_is_white(st):
    for c in st:
        if not char_is_white(c):
            return False
    return True

#Tokenization (lexical) rules
#
#   sequences of " ", newline, tabs are one token with all other chars delimiting
#   ) ( and , are always their own single char token
#   sequences a-z, a-Z and _ are one token with all other chars delimiting
#   sequences of all other chars not yet mentioned are one token
#       (with a-z,A-Z,_,(,),, delimiting)

def tokenize(st):
    ret = []

    s = "none"
    for c in st:
        if char_is_white(c):
            ct = "white"
        #FIXME: Handle string literals properly
        elif str.isalnum(c) or c == "_" or c == "\"":
            ct = "alpha"
        elif c == "(" or c == ")" or c == ",":
            ct = c
        else:
            ct = "special"

        if s != ct:
            if (s != "none"):
                ret.append(tok)
            s = ct
            tok = c
        else:
            tok += c

    if s != "none":
        ret.append(tok)

    return ret

for line in sys.stdin:
    line_nr += 1
    #ditch comments
    linem = re.sub(r"//.*$", r"", line)
    #Keep leading whitespace on each line
    #obufs[-1].append(re.sub(r"^([ \t]*).*$", r"\1", line))
    if (state == "include"):
        state = "off";
    for token in tokenize(linem):
        #sys.stderr.write(token + "(" + state + ") ")
        if (string_is_white(token)):
            if state == "off":
                obufs[-1].append(token);
            continue;
        qtoken = "\'" + token + "\'"
        if (state == "off") :
            if token == "#":
                state = "#d"
            else:
                obufs[-1].append(token);
        elif state == "#d":
            if token == "for":
                state = "find"
                finds.append([]);
                replace = []
                obufs.append([])
            elif token == "endfor":
                state = "off"
                parse_error(len(obufs) == 1, "\'#for\'", qtoken)
                obuf = obufs.pop()
                find = finds.pop()
                for r in replaces.pop():
                    nextglue = False;
                    for t in obuf:
                        if nextglue and string_is_white(t):
                            continue;
                        fi = 0;
                        for rv in r:
                            toadd = rv;
                            preglue = False;
                            postglue = False;
                            if (t == "_" + find[fi] + "_"):
                                postglue = True;
                                preglue = True;
                            elif (t == find[fi] + "_"):
                                postglue = True;
                            elif (t == "_" + find[fi]):
                                preglue = True;
                            elif (t != find[fi]):
                                toadd = t;
                                fi = fi + 1;
                                continue;
                            break;

                        if nextglue or preglue:
                            while (string_is_white(obufs[-1][-1])):
                                del obufs[-1][-1]
                            obufs[-1][-1] += toadd;
                        else:
                            obufs[-1].append(toadd)
                        nextglue = postglue
            else:
                state = "off"
                obufs[-1].append("#");
                obufs[-1].append(token);
        elif state == "find":
            parse_error(is_keyword(token), "identifier", qtoken)
            finds[-1].append(token)
            state = "("
        elif state == "(":
            if (token == ":"):
                state = "find";
            else:
                parse_error(token != "(", "\'(\'", qtoken)
                replace.append([]);
                state = "replaces"
        elif state == "replaces":
            if token == ")":
                replaces.append(replace)
                state = "off"
            else:
                parse_error(is_keyword(token), "identifier or \')\'", qtoken)
                replace[-1].append(token)
                state= ","
        elif state == ",":
            if (token == ":"):
                state = "replaces"
            elif token == ")":
                replaces.append(replace)
                state = "off"
            else:
                replace.append([])
                parse_error(token != ",", "\',\' or \')\"", qtoken)
                state = "replaces"

parse_error(state == "find", "identifier", "EOF")
parse_error(state == "(", "\'(\'", "EOF")
parse_error(state == "replaces", "identifier or \')\'", "EOF")
parse_error(len(obufs) != 1, "#endfor", "EOF")

for token in obufs[-1]:
    sys.stdout.write(token)
